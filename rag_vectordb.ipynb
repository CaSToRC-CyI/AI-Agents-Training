{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing a Vector Database for our RAG application\n",
    "\n",
    "This notebook demonstrates how to use a vector database to build a Retrieval-Augmented Generation (RAG) pipeline. Follow the steps below to understand the process of setting up the pipeline, indexing documents, and retrieving answers to queries using OpenAI's GPT models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "This section clones the relevant data we are going to use in this notebook, while also installed all the relevant packages.\n",
    "\n",
    "\n",
    "**NOTE: Make sure to change the notebook runtime to T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1284,
     "status": "ok",
     "timestamp": 1753085220573,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "uScNqZuCj7Bz",
    "outputId": "56f04e8e-c4e7-4b9e-9c43-a24aef1a4410"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/CaSToRC-CyI/AI-Agents-Training.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1753085220591,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "4N5gGzBikAi_",
    "outputId": "f5e05a46-ce3e-4589-ef1e-b3fd1d9158df"
   },
   "outputs": [],
   "source": [
    "%cd ./AI-Agents-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1753085221213,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "a2Rdjph0iaaJ",
    "outputId": "d8bffd44-711f-40aa-c42b-226b3fb3c0a3"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "uv pip install haystack-ai\n",
    "uv pip install milvus_haystack\n",
    "uv pip install pymilvus\n",
    "uv pip install python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 8772,
     "status": "ok",
     "timestamp": 1753085229994,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "-jUgcLW0idZq"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import DOCXToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "\n",
    "from milvus_haystack import MilvusDocumentStore\n",
    "from milvus_haystack.milvus_embedding_retriever import MilvusEmbeddingRetriever\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.dataclasses import ChatMessage\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Open-AI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23350,
     "status": "ok",
     "timestamp": 1753085253365,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "H7Uy9jYxtj8l",
    "outputId": "f9e9c3db-06f8-42cd-80e4-496b84579f5d"
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to Haystack Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haystack uses these abstraction called *Documents*. They can hold text, tables, and binary data.\n",
    "\n",
    "They have the following unique features:\n",
    "\n",
    "- Unique ID for each document.\n",
    "- Multiple content types are supported.\n",
    "- Custom metadata and scoring for advanced document management.\n",
    "- Optional embeddings for AI-based applications\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Document(metaclass=_BackwardCompatible):\n",
    "    id: str = field(default=\"\")\n",
    "    content: Optional[str] = field(default=None)\n",
    "    blob: Optional[ByteStream] = field(default=None)\n",
    "    meta: Dict[str, Any] = field(default_factory=dict)\n",
    "    score: Optional[float] = field(default=None)\n",
    "    embedding: Optional[List[float]] = field(default=None)\n",
    "    sparse_embedding: Optional[SparseEmbedding] = field(default=None)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1753085253377,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "t5YtLuUStmfq"
   },
   "outputs": [],
   "source": [
    "DOCUMENTS_DIR = Path(\"./dummy_data/documents_dir\")\n",
    "FILES = [file.resolve() for file in DOCUMENTS_DIR.rglob(\"*\") if file.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1753085253999,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "QY-6fN4BuY3B",
    "outputId": "e9921fb8-86b1-4cce-b51f-e1cf7e0b1d3e"
   },
   "outputs": [],
   "source": [
    "converter = DOCXToDocument()\n",
    "\n",
    "result = converter.run(sources=FILES)\n",
    "print(f'{result[\"documents\"][0].meta[\"file_path\"]}')\n",
    "print(result[\"documents\"][0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Vector Database\n",
    "\n",
    "Set up the Milvus vector database to store document embeddings. The `drop_old` parameter ensures any existing data is cleared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1753085295192,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "0AuTkCodBINC"
   },
   "outputs": [],
   "source": [
    "connection_args={\"uri\": \"./rag_vectordb.db\"}\n",
    "document_store = MilvusDocumentStore(\n",
    "    connection_args=connection_args,\n",
    "    drop_old=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Documents and performing RAG\n",
    "\n",
    "Create a pipeline to process, clean, split, embed, and store the documents in the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Indexing components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use our documents we need to perform 5 things:\n",
    "\n",
    "1. Turn them into compatible Haystack *Documents*.\n",
    "2. Clean each Document using Haystack's `DocumentCleaner`. This removes any whitespaces, empty lines, specified substrings, regexes and so on.\n",
    "3. Then we split our documents into *smaller chunks*. We can define various split methods and length.\n",
    "4. Turn them into embeddings with an *embedder*.\n",
    "5. Store them in a Haystack *Document Store* so they can be accessed later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9592,
     "status": "ok",
     "timestamp": 1753085421825,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "8-VBtjAbBIvU",
    "outputId": "36a1e3b6-8c04-4cde-fa7a-19d6e175316a"
   },
   "outputs": [],
   "source": [
    "# Initialize the indexing pipeline\n",
    "indexing_pipeline = Pipeline()\n",
    "\n",
    "# Add each component to the pipeline\n",
    "indexing_pipeline.add_component(\"converter\", DOCXToDocument())\n",
    "indexing_pipeline.add_component(\"cleaner\", DocumentCleaner())\n",
    "indexing_pipeline.add_component(\"splitter\", DocumentSplitter(split_by=\"sentence\", split_length=2))\n",
    "indexing_pipeline.add_component(\"embedder\", OpenAIDocumentEmbedder())\n",
    "indexing_pipeline.add_component(\"writer\", DocumentWriter(document_store))\n",
    "\n",
    "# Connect each component\n",
    "indexing_pipeline.connect(\"converter\", \"cleaner\")\n",
    "indexing_pipeline.connect(\"cleaner\", \"splitter\")\n",
    "indexing_pipeline.connect(\"splitter\", \"embedder\")\n",
    "indexing_pipeline.connect(\"embedder\", \"writer\")\n",
    "\n",
    "# Run the Pipeline\n",
    "indexing_pipeline.run({\"converter\": {\"sources\": FILES}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the retrieval using a Vector Database\n",
    "\n",
    "In this cell below, we can see the output of our retriever based on our query.\n",
    "\n",
    "The two components we need is the embedder that turns the query into an embedding, and then the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1753085451437,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "__9Yvu72CIJj",
    "outputId": "75aa0da6-585e-46d8-cae0-6f28902bfdf5"
   },
   "outputs": [],
   "source": [
    "question = \"Tell me a bit about QuantumStream\"  # You can replace it with your own question.\n",
    "\n",
    "retrieval_pipeline = Pipeline()\n",
    "retrieval_pipeline.add_component(\"embedder\", OpenAITextEmbedder())\n",
    "retrieval_pipeline.add_component(\"retriever\", MilvusEmbeddingRetriever(document_store=document_store, top_k=3))\n",
    "retrieval_pipeline.connect(\"embedder\", \"retriever\")\n",
    "\n",
    "retrieval_results = retrieval_pipeline.run({\"embedder\": {\"text\": question}})\n",
    "\n",
    "for doc in retrieval_results[\"retriever\"][\"documents\"]:\n",
    "    print(doc.content)\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Template for user\n",
    "\n",
    "This prompt template will be used by our LLM to generate a response based on our Query.\n",
    "\n",
    "Specifically, the LLM will read this text from top to bottom:\n",
    "\n",
    "- It will read the task, which is to respond to the user's query using the **provided context**.\n",
    "- It will then read some **General Guidelines**.\n",
    "- Then it will read the **provided context**.\n",
    "- And finally it will read the **user's query**.\n",
    "\n",
    "You can see that we pass the **context** and **user's query** through this template. This is why we use a prompt builder later on. This component constructs prompts dynamically by processing chat messages.\n",
    "\n",
    "Specifically, the *ChatPromptBuilder* component creates prompts using static or dynamic templates written in Jinja2 syntax, by processing a list of chat messages. The templates contain placeholders like {{ variable }} that are filled with values provided during runtime. You can use it for static prompts set at initialization or change the templates and variables dynamically while running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1753085473942,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "St-aK-SvCRKM",
    "outputId": "989cb78d-9959-4c41-b621-a9f78cb53a84"
   },
   "outputs": [],
   "source": [
    "template = [\n",
    "    ChatMessage.from_user(\n",
    "        \"\"\"\n",
    "Respond to the User Query using the provided Context.\n",
    "\n",
    "General Guidelines:\n",
    "    - Ensure citations are concise and directly related to the information provided.\n",
    "    - If the answer is not found in the context, state this clearly instead of making assumptions.\n",
    "    - If the answer comes from several sources, make sure to cite every one of them, including their Source Filename, Source Chapter and Source Page.\n",
    "    - If information is region-specific, clarify which region it pertains to.\n",
    "    - Respond in the same language as the user’s query.  \n",
    "    - Do not use emojis.\n",
    "    - Be professional and punctual\n",
    "    - *Avoid* writing a conclusion or a follow-up at the end of each response unless you were asked to.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "User's Query: {{query}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual RAG pipeline we have the following components:\n",
    "\n",
    "- The *text_embedder* which takes the user's query and turns it into embeddings\n",
    "- The *retriever* which retrieves the relevant documents. The retriever is **different** now. We are using one that is compatible with our Vector Database.\n",
    "- The *chat_generator* which is our LLM\n",
    "- The *promt_builder* which was explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG pipeline\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", OpenAITextEmbedder())\n",
    "rag_pipeline.add_component(\"retriever\", MilvusEmbeddingRetriever(document_store=document_store, top_k=3))\n",
    "rag_pipeline.add_component(\"prompt_builder\", ChatPromptBuilder(template=template))\n",
    "rag_pipeline.add_component(\"llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\"))\n",
    "\n",
    "# Connect the input/output of each component\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever\", \"prompt_builder\")\n",
    "rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform RAG on our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change the question to something else.\n",
    "\n",
    "Our documents contain information about the following topics:\n",
    "\n",
    "- Annual Hackathon the company is organizing\n",
    "- Cybersecurity Awareness Month\n",
    "- Employee Recognition Program\n",
    "- New Office Layout Plan\n",
    "- Office layout redesign plan\n",
    "- Product X Launch Timeline\n",
    "- Product Y Launch Timeline\n",
    "- QuantumStream product CLI Usage\n",
    "- QuantumStream product Data Encryption feature\n",
    "- QuantumStream product Plugin System\n",
    "- QuantumStream product REST API documentation\n",
    "- QuantumStream product Scheduler feature\n",
    "- QuantumStream product Scheduling tasks\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to ask anything relating to these topics.\n",
    "\n",
    "**Suggested prompts:**\n",
    "\n",
    "- \"Whats the purpose of the new office layout? Are we loosing our desks??\"\n",
    "- \"I am a new employee at the company. Onboard me about the QuantumStream product.\"\n",
    "- \"I cannot find the relevant email about the Hackathon, can you tell me more details about it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2602,
     "status": "ok",
     "timestamp": 1753085478947,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "4oP0KKoxCXFP",
    "outputId": "6214fe71-a4f6-42e3-8cad-a5d6849bf674"
   },
   "outputs": [],
   "source": [
    "question = \"Tell me a bit about QuantumStream\"  # You can replace it with your own question.\n",
    "results = rag_pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"query\": question}})\n",
    "formatted_text = response[\"llm\"][\"replies\"][0].text\n",
    "\n",
    "wrapped_text = \"\\n\".join(\n",
    "    textwrap.fill(line, width=120, subsequent_indent=\"  \") if line.strip() else line\n",
    "    for line in formatted_text.splitlines()\n",
    ")\n",
    "\n",
    "print(wrapped_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOgnacqgWL73uCmRWgaPl7T",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
