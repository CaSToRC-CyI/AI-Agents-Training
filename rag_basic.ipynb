{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a basic RAG pipeline with Haystack\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a method that combines information retrieval with generative models to provide accurate and context-aware respones.\n",
    "\n",
    "It is particularly useful for tasks requiring domain-specific knowledge or large-scale document retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section clones the relevant data we are going to use in this notebook, while also installed all the relevant packages.\n",
    "\n",
    "\n",
    "**NOTE: Make sure to change the notebook runtime to T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1481,
     "status": "ok",
     "timestamp": 1753079702369,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "PwuT0l91UMj5",
    "outputId": "f72d41b6-42b3-4cc5-ade7-2a0383db9aaf"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/CaSToRC-CyI/AI-Agents-Training.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1753079702373,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "pHBMCIH7S4-s",
    "outputId": "46dc2166-8481-44c4-87ee-f1dda7e41191"
   },
   "outputs": [],
   "source": [
    "%cd ./AI-Agents-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1753079703254,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "lApOubh5mHFt",
    "outputId": "4480b1ac-9dee-4af2-8a81-45bc6a98f3b1"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "uv pip install haystack-ai\n",
    "uv pip install datasets -U\n",
    "uv pip install \"sentence-transformers>=4.1.0\"\n",
    "uv pip install huggingface_hub -U\n",
    "uv pip install python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 38536,
     "status": "ok",
     "timestamp": 1753079741794,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "hHOwsEtUNG8h"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack import Document\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.converters import DOCXToDocument\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Open-AI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69185,
     "status": "ok",
     "timestamp": 1753079810991,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "ZGwCrrTCNIwT",
    "outputId": "53c380c8-53c6-45ac-eeb0-135c2606e330"
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to Haystack Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haystack uses these abstraction called *Documents*. They can hold text, tables, and binary data.\n",
    "\n",
    "They have the following unique features:\n",
    "\n",
    "- Unique ID for each document.\n",
    "- Multiple content types are supported.\n",
    "- Custom metadata and scoring for advanced document management.\n",
    "- Optional embeddings for AI-based applications\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Document(metaclass=_BackwardCompatible):\n",
    "    id: str = field(default=\"\")\n",
    "    content: Optional[str] = field(default=None)\n",
    "    blob: Optional[ByteStream] = field(default=None)\n",
    "    meta: Dict[str, Any] = field(default_factory=dict)\n",
    "    score: Optional[float] = field(default=None)\n",
    "    embedding: Optional[List[float]] = field(default=None)\n",
    "    sparse_embedding: Optional[SparseEmbedding] = field(default=None)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert our files do Haystack Documents, we need to use a *converter*. In our case, since we only have .docx documents, we can go ahead and use Haystack's DOCXToDocument converter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENTS_DIR = Path(\"./dummy_data/documents_dir\")\n",
    "FILES = [file.resolve() for file in DOCUMENTS_DIR.rglob(\"*\") if file.is_file()]\n",
    "converter = DOCXToDocument()\n",
    "\n",
    "docs = []\n",
    "for file in FILES:\n",
    "    result = converter.run(sources=[file])\n",
    "    docs.extend(result[\"documents\"])  # Append the converted documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect a sample document from our database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1753079812655,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "mcWeRIgwNMWS",
    "outputId": "8d4e89e6-710e-454e-c954-5751d3956ecd"
   },
   "outputs": [],
   "source": [
    "print(docs[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Documents and performing RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Indexing components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use our documents we need to perform 2 things:\n",
    "\n",
    "1. Turn them into embeddings with an *embedder*.\n",
    "2. Store them in a Haystack *Document Store* so they can be accessed later on.\n",
    "\n",
    "For our simple use-case, we will use an OpenAI document embedder to extract embeddings, and then we will store them in an *InMemoryDocumentStore*. Basically we are storing them in our system's RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1753079812662,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "1AlEcKmCpKGw"
   },
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore()\n",
    "doc_embedder = OpenAIDocumentEmbedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings and store to Document Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and run the cell below to begin calculating the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3197,
     "status": "ok",
     "timestamp": 1753079816038,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "6PzhXdYlNPMf",
    "outputId": "7c35e281-c1a3-48e2-fb9e-b74885c5f492"
   },
   "outputs": [],
   "source": [
    "docs_with_embeddings = doc_embedder.run(docs)\n",
    "document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
    "print(f\"Stored {len(docs_with_embeddings['documents'])} documents with embeddings in the document store.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Template for user\n",
    "\n",
    "This prompt template will be used by our LLM to generate a response based on our Query.\n",
    "\n",
    "Specifically, the LLM will read this text from top to bottom:\n",
    "\n",
    "- It will read the task, which is to respond to the user's query using the **provided context**.\n",
    "- It will then read some **General Guidelines**.\n",
    "- Then it will read the **provided context**.\n",
    "- And finally it will read the **user's query**.\n",
    "\n",
    "You can see that we pass the **context** and **user's query** through this template. This is why we use a prompt builder later on. This component constructs prompts dynamically by processing chat messages.\n",
    "\n",
    "Specifically, the *ChatPromptBuilder* component creates prompts using static or dynamic templates written in Jinja2 syntax, by processing a list of chat messages. The templates contain placeholders like {{ variable }} that are filled with values provided during runtime. You can use it for static prompts set at initialization or change the templates and variables dynamically while running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1753079816059,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "zXe8Du2HNQaM",
    "outputId": "2a431c63-2ca5-4cc0-bc98-42db16c803ed"
   },
   "outputs": [],
   "source": [
    "template = [\n",
    "    ChatMessage.from_user(\n",
    "        \"\"\"\n",
    "Respond to the User Query using the provided Context.\n",
    "\n",
    "General Guidelines:\n",
    "    - Ensure citations are concise and directly related to the information provided.\n",
    "    - If the answer is not found in the context, state this clearly instead of making assumptions.\n",
    "    - If the answer comes from several sources, make sure to cite every one of them, including their Source Filename, Source Chapter and Source Page.\n",
    "    - If information is region-specific, clarify which region it pertains to.\n",
    "    - Respond in the same language as the user’s query.  \n",
    "    - Do not use emojis.\n",
    "    - Be professional and punctual\n",
    "    - *Avoid* writing a conclusion or a follow-up at the end of each response unless you were asked to.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual RAG pipeline we have the following components:\n",
    "\n",
    "- The *text_embedder* which takes the user's query and turns it into embeddings\n",
    "- The *retriever* which retrieves the relevant documents\n",
    "- The *chat_generator* which is our LLM\n",
    "- The *promt_builder* which was explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1753079816329,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "lQDnTWp6NSGQ"
   },
   "outputs": [],
   "source": [
    "text_embedder = OpenAITextEmbedder()\n",
    "retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
    "prompt_builder = ChatPromptBuilder(template=template)\n",
    "\n",
    "# Initialize RAG pipeline\n",
    "basic_rag_pipeline = Pipeline()\n",
    "\n",
    "basic_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "basic_rag_pipeline.add_component(\"retriever\", retriever)\n",
    "basic_rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "basic_rag_pipeline.add_component(\"llm\", chat_generator)\n",
    "\n",
    "# Connect the input/output of each component\n",
    "basic_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "basic_rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "basic_rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We connect each component by defining the inputs and outputs.\n",
    "\n",
    "For example:\n",
    "\n",
    "- The *text_embedder* will take the user's query as an input and output *embeddings*. These embeddings will be the input of the *retriever*. The *retriever* will take that input as *query_embedding* and output a list of *documents* that are similar to that query.\n",
    "- Then, the *retriever* outputs the documents we mentioned, and pass them into our *prompt_builder*.\n",
    "- Finally, the *prompt_builder* will output the prompt and send it as an input to the *llm*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform RAG on our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change the question to something else.\n",
    "\n",
    "Our documents contain information about the following topics:\n",
    "\n",
    "- Annual Hackathon the company is organizing\n",
    "- Cybersecurity Awareness Month\n",
    "- Employee Recognition Program\n",
    "- New Office Layout Plan\n",
    "- Office layout redesign plan\n",
    "- Product X Launch Timeline\n",
    "- Product Y Launch Timeline\n",
    "- QuantumStream product CLI Usage\n",
    "- QuantumStream product Data Encryption feature\n",
    "- QuantumStream product Plugin System\n",
    "- QuantumStream product REST API documentation\n",
    "- QuantumStream product Scheduler feature\n",
    "- QuantumStream product Scheduling tasks\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to ask anything relating to these topics.\n",
    "\n",
    "**Suggested prompts:**\n",
    "\n",
    "- \"Whats the purpose of the new office layout? Are we loosing our desks??\"\n",
    "- \"I am a new employee at the company. Onboard me about the QuantumStream product.\"\n",
    "- \"I cannot find the relevant email about the Hackathon, can you tell me more details about it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3779,
     "status": "ok",
     "timestamp": 1753079820154,
     "user": {
      "displayName": "Marios Constantinou",
      "userId": "08118670076332792273"
     },
     "user_tz": -180
    },
    "id": "szzh7B5INUUs",
    "outputId": "06cb9d3c-1329-4cb2-ee74-833689bf97f2"
   },
   "outputs": [],
   "source": [
    "question = \"I am a new employee at the company. Onboard me about the QuantumStream product.\" # Feel free to change this question\n",
    "\n",
    "response = basic_rag_pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}})\n",
    "formatted_text = response[\"llm\"][\"replies\"][0].text\n",
    "wrapped_text = \"\\n\".join(\n",
    "    textwrap.fill(line, width=120, subsequent_indent=\"  \") if line.strip() else line\n",
    "    for line in formatted_text.splitlines()\n",
    ")\n",
    "\n",
    "print(wrapped_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyON7iiLmrYaPfbHl4IbRyS6",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
